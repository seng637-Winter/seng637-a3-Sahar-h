**SENG 637 - Dependability and Reliability of Software Systems**

**Lab. Report #3 â€“ Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group \#:      | 15 |
| -------------- | --- |
| Student Names: |Mehrnaz Senobari|
|                |Sina Salimian|
|                |Sahar Hajjar Zadeh|
|                |Zar Safari|
|                |Fatemeh Ghaffarpour|
|                |Alireza Esmaeili|

# 1. Introduction
White-box testing is a way of checking software where we look inside the code itself, unlike black-box testing, which only looks at what the software does from the outside. With white-box testing, we can see exactly which parts of the code the tests are checking and which parts they're not.

This kind of testing is really useful because it lets us make sure that all the different parts of the code are tested. We can find problems or parts of the code that might not work right under certain conditions.

We're using white-box testing on a project called JFreeChart. JFreeChart is a tool that lets people add charts, like bar graphs or pie charts, to their Java applications easily. We already started testing JFreeChart in our last assignment, and now we want to do even more tests to make sure we cover more of the code.

When we talk about covering more of the code, we're talking about "code coverage." Code coverage is just a way to measure how much of the code we have tested. There are a few different ways to measure this:

**1. Statement Coverage:** This checks if we've tested every part of the code.

**2. Branch Coverage:** This makes sure we've tested every possible path or choice in the code, like all the "if" and "else" decisions.
**3. Condition Coverage:** This goes deeper into the decisions and checks every possible true or false condition.

**4. Path Coverage:** This is the most detailed, checking every single way we could go through the code when it runs.

By using these measures on the JFreeChart project, we want to make our tests better and make sure the JFreeChart tool works well and reliably for everyone who uses it.


# 2. Manual data-flow coverage calculations for X and Y methods

# 3. A detailed description of the testing strategy for the new unit test

# 4. A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage

# 5. A detailed report of the coverage achieved of each class and method (a screen shot from the code cover results in green and red color would suffice)

# 6. Pros and Cons of coverage tools used and Metrics you report

# 7. A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation

# 8. A discussion on how the team work/effort was divided and managed

# 9. Any difficulties encountered, challenges overcome, and lessons learned from performing the lab

# 10. Comments/feedback on the lab itself
